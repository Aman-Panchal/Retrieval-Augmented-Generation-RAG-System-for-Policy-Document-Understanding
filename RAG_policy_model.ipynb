{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTLiGb0paFwx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-ORV1HwUGs8R8vWlaRYLdAzJfhxLH9NYWyb5GDGGm9Il4JLsPWQX5L1I8A9hR\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Setup Environment & Dependencies**"
      ],
      "metadata": {
        "id": "a0LVQAwEj9mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Install libraries\n",
        "!pip install langchain openai tiktoken chromadb faiss-cpu unstructured\n",
        "!pip install sentence-transformers  # or\n",
        "!pip install openai  # if using OpenAI embeddings\n",
        "!pip install langchain openai chromadb tiktoken\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "8F7iZIBmB8_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "hPcswe0tExci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Load and Preprocess the JSON Data**"
      ],
      "metadata": {
        "id": "4dpdTyoMbEwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Load the structured policy\n",
        "with open('/content/drive/MyDrive/RAG/json/structured_policy1.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert to LangChain Documents\n",
        "docs = []\n",
        "for section in data:\n",
        "    metadata = {\"section\": section[\"section\"]}\n",
        "    content = section[\"content\"]\n",
        "    docs.append(Document(page_content=content, metadata=metadata))\n"
      ],
      "metadata": {
        "id": "1-7wV2IHkHTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Embed Documents and Store in Vector DB (Chroma)**"
      ],
      "metadata": {
        "id": "jvDcx-0RkXLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding,\n",
        "    persist_directory=\"/content/chroma_store\"\n",
        ")\n",
        "vectordb.persist()\n"
      ],
      "metadata": {
        "id": "CcN1blDCkWvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Setup Retriever and LLM Chain**"
      ],
      "metadata": {
        "id": "gttYQTP1kdm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
        "\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "zxYuOLUjkKb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Ask Questions**"
      ],
      "metadata": {
        "id": "M0DpDLI1kkI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"You are a helpful assistant.Answer ONLY from the provided transcript context. If the context is insufficient, just say you dont know. Question:\"\n",
        "query = template + \"What is the purpose of the policy?\"\n",
        "result = rag_chain(query)\n",
        "\n",
        "print(\"Answer:\", result['result'])\n",
        "for doc in result['source_documents']:\n",
        "    print(\"\\nSource:\", doc.metadata['section'])\n",
        "    print(doc.page_content[:300])\n"
      ],
      "metadata": {
        "id": "aba5e3iXklix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a Chain**"
      ],
      "metadata": {
        "id": "mlLwJSCLnJnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install -q langchain faiss-cpu sentence-transformers huggingface_hub\n",
        "\n",
        "# Step 2: Imports\n",
        "import json\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "# Step 3: Load and parse structured_policy1.json\n",
        "def load_policy_json(json_path):\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    docs = []\n",
        "    for section in data:\n",
        "        content = section[\"content\"]\n",
        "        metadata = {\"section\": section.get(\"section\", section.get(\"title\", \"N/A\"))}\n",
        "        docs.append(Document(page_content=content, metadata=metadata))\n",
        "    return docs\n",
        "\n",
        "json_path = \"/content/drive/MyDrive/RAG/json/structured_policy1.json\"\n",
        "documents = load_policy_json(json_path)\n",
        "\n",
        "# Step 4: Text splitting\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "split_docs = splitter.split_documents(documents)\n",
        "\n",
        "# Step 5: Embeddings + Vector Store\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", k=3)\n",
        "\n",
        "# Step 6: Setup RAG chain with HuggingFaceHub LLM (Flan-T5)\n",
        "from huggingface_hub import login\n",
        "login(\"YOUR_HUGGINGFACE_TOKEN\")  # üîê paste your token here\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-base\",\n",
        "    model_kwargs={\"temperature\": 0.2, \"max_length\": 512}\n",
        ")\n",
        "\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type=\"stuff\"\n",
        ")\n",
        "\n",
        "# Step 7: Ask questions interactively\n",
        "while True:\n",
        "    user_query = input(\"\\nüîç Ask your policy question (or type 'exit' to quit): \")\n",
        "    if user_query.lower() == \"exit\":\n",
        "        break\n",
        "    result = rag_chain(user_query)\n",
        "    print(\"\\nüß† Answer:\")\n",
        "    print(result['result'])\n",
        "    print(\"\\nüìÑ Sources:\")\n",
        "    for doc in result[\"source_documents\"]:\n",
        "        print(f\"\\n‚Üí Section: {doc.metadata.get('section', 'N/A')}\")\n",
        "        print(doc.page_content[:300] + \"...\")\n"
      ],
      "metadata": {
        "id": "4ZdZkXZLmWKx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}